{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Spark\n",
    " \n",
    "In this lesson we will talk about spark and his components.\n",
    "Nesta aula iremos falar sobre spark e os seus componentes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is Spark?\n",
    "\n",
    "Apache spark is  unified computing engine and a set of libraries for paralel data processing  on computer clusters. \n",
    "It supports multiple programming languages and include multiple libraries for diverse tasks such as: SQL, Streaming, Machine Learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Why Spark:\n",
    "Single machines do not have enough power and resources to perform computations on huge amounts of information\n",
    "(or the user probably does not have time to wait for the computation to finish)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark Applications\n",
    "\n",
    "\n",
    "Consists of a **driver process** and a set of **executor processes** \n",
    "\n",
    "The **driver process** runs the main function  and is responsible for 3 things:\n",
    "* Maintaining information about Spark Application.\n",
    "* Responding to a user's program or input.\n",
    "* Analyzing, Distributing, and scheduling work across the executors\n",
    "\n",
    "The driver process mantains all the relevant information during the lifetime of the application.\n",
    "\n",
    "**Executors** are responsible for carrying out the work that the driver assigns them.\n",
    "\n",
    "Each **executor** is responsible for **two** things:\n",
    "* Executing the code assign to it by the driver.\n",
    "* Reporting the state of the computation on that executor back to the driver node.\n",
    "\n",
    "      \n",
    "Inserir aqui imagem a explicar\n",
    "\n",
    "Spark has two modes, **cluster mode** and **local mode**:\n",
    "* Cluster Mode -> In cluster mode the driver and the executors are simply processes, they can live on the same machine or different machines\n",
    "* Local Mode -> The driver and the executors run as threads on individual computer \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**There is a Spark Session object available to the user, which is the enterance point to running Spark code.**\n",
    "\n",
    "**Spark translates into the code that it then can run on the executor.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a Spark Session to send commands into the spark application.\n",
    "\n",
    "Spark session instance is  the way Spark executes user-defined manipulation across the cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'spark' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-40efee8179dd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mspark\u001b[0m \u001b[0;31m# shows the spark session object\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'spark' is not defined"
     ]
    }
   ],
   "source": [
    "spark # shows the spark session object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'spark' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-015c2ba8c1af>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmyRange\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoDF\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"number\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Dataframe with one column containing 1000 rows with numbers from 0 to 999\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'spark' is not defined"
     ]
    }
   ],
   "source": [
    "myRange = spark.range(1000).toDF(\"number\") # Dataframe with one column containing 1000 rows with numbers from 0 to 999"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataFrames\n",
    "\n",
    "Dataframe represents a table of data with rows and columns.  \n",
    "The list that defines the columns and his types is called schema.  \n",
    "Spark Dataframe can span thousands of computers.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'myRange' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-aa1a2ba6dfcb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmyRange\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'myRange' is not defined"
     ]
    }
   ],
   "source": [
    "myRange"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Partitions\n",
    "\n",
    "To allow every executor execute tasks in parallel  Spark breaks up the data in chunks called partitions. \n",
    "A partition is a collection of rows that sit on one physical machine in the cluster.\n",
    "\n",
    "**IMPORTANT**\n",
    "\n",
    "We do not manipulate partitions manually or individually. We simply specify high level transformations of the data in physical partitions  Spark determines how this work will execute on the cluster\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The core data structures are immutable this means that they cannot be changed after they are created.\n",
    "\n",
    "We need to instruct spark how to modify it. These instructions is called  transformations.\n",
    "\n",
    "Transformations is how we express business logic using Spark there are two types:\n",
    "\n",
    "Those who specify narrow dependencies and those  that specify wide dependencies.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "divisBy2 = myRange.where(\"number % 2 = 0\") # Perform a simple transformation to find all even numbers in our Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We only specifyan abstract transformation. spark not act on transformations until we call action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lazy evaluation\n",
    "\n",
    "Lazy evaluation means that spark will wait until the last moment to execute the graph of computation instructions.\n",
    "\n",
    "In Spark instead of modifying the data immediately when you express some operation, we build up a plan of transformations to apply to our dataframe.\n",
    "\n",
    "Then spark optimize this entire flow end to end\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Actions\n",
    "\n",
    "To trigger computation we run an action.  \n",
    "An action instructs spark to compute a result from a series of transformations.\n",
    "\n",
    "* Actions to view in the console \n",
    "\n",
    "* Actions to collect data to native objects in the respective language\n",
    "\n",
    "* Actions to write to out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'divisBy2' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-2386307e7a39>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdivisBy2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'divisBy2' is not defined"
     ]
    }
   ],
   "source": [
    "divisBy2.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
